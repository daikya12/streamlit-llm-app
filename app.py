from dotenv import load_dotenv
load_dotenv()

# app.py
import os
import streamlit as st

# Lesson8ã‚¹ã‚¿ã‚¤ãƒ«: ChatOpenAI ã¨ schema ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

# ======================
# è¨­å®šï¼ˆãƒ¢ãƒ‡ãƒ«ãªã©ï¼‰
# ======================
MODEL_NAME = "gpt-4o-mini"   # è»½é‡ãƒ»ä½ã‚³ã‚¹ãƒˆã§ååˆ†
TEMPERATURE = 0.3            # å†ç¾æ€§é‡è¦–

# ======================
# å½¹å‰²ï¼ˆå°‚é–€å®¶ï¼‰â†’ ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å®šç¾©
# A,B ã¯è¦ä»¶å›ºå®šã€‚+Î±ã¯ä»»æ„ã§è¿½åŠ 
# ======================
ROLE_SYSTEM_PROMPTS = {
    "ã‚³ãƒ¼ãƒ’ãƒ¼ã®å°‚é–€å®¶ï¼ˆAï¼‰": (
        "ã‚ãªãŸã¯ä¸–ç•Œãƒ¬ãƒ™ãƒ«ã®ã‚³ãƒ¼ãƒ’ãƒ¼å°‚é–€å®¶ï¼ˆãƒˆãƒƒãƒ—ãƒãƒªã‚¹ã‚¿å…¼ãƒ­ãƒ¼ã‚¹ã‚¿ãƒ¼ï¼‰ã§ã™ã€‚"
        "è³ªå•è€…ã®ç›®çš„ã¨å™¨å…·ãƒ»è±†ã®æ¡ä»¶ã‚’ç¢ºèªã—ã€å®Ÿç”¨çš„ã§å†ç¾æ€§ã®é«˜ã„ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æ—¥æœ¬èªã§ç«¯çš„ã«è¿”ã—ã¦ãã ã•ã„ã€‚"
        "\n\n"
        "ã€å›ç­”ãƒãƒªã‚·ãƒ¼ã€‘\n"
        "1) ã¾ãšå‰æã®ç¢ºèªï¼ˆè±†ã®ç¨®é¡/ç„™ç…åº¦/æŒ½ç›®/æŠ½å‡ºå™¨å…·/æ¹¯æ¸©/ç²‰é‡/æ³¨æ¹¯æ™‚é–“/ç‹™ã†é¢¨å‘³ï¼‰ã‚’1è¡Œã§è¦ç´„ã€‚\n"
        "2) æ¨å¥¨ãƒ¬ã‚·ãƒ”ï¼ˆç²‰é‡, æ¹¯é‡, æ¯”ç‡, æ¹¯æ¸©, æŒ½ç›®ã®ç›®å®‰, æŠ½å‡ºæ™‚é–“, æ³¨æ¹¯ã‚¹ãƒ†ãƒƒãƒ—ï¼‰ã‚’æ•°å€¤å…¥ã‚Šã§æç¤ºã€‚\n"
        "3) å‘³ã®å¾®èª¿æ•´Tipsï¼ˆé…¸ãƒ»ç”˜ã¿ãƒ»ãƒœãƒ‡ã‚£ã®èª¿æ•´æ–¹æ³•ï¼‰ã‚’ç®‡æ¡æ›¸ãã§3å€‹ä»¥å†…ã€‚\n"
        "4) å®¶åº­ç’°å¢ƒã§ã®è½ã¨ã—ç©´ï¼ˆã‚¹ã‚±ãƒ¼ãƒ«é™¤å»/æ¹¯æ¸©ç²¾åº¦/æŒ½ç›®ã®å†ç¾ï¼‰ã‚’1è¡Œã§æ³¨æ„å–šèµ·ã€‚\n"
        "5) å¯èƒ½ãªã‚‰ä»£æ›¿æ¡ˆï¼ˆä»–å™¨å…·ã‚„æµ…æ·±ç„™ç…ã®åˆ‡æ›¿ï¼‰ã‚’1ã¤ã€‚\n"
        "\n"
        "ã€æ•°å€¤ã‚¬ã‚¤ãƒ‰ï¼ˆç›®å®‰ï¼‰ã€‘\n"
        "- ãƒ‰ãƒªãƒƒãƒ—æ¯”ç‡: 1:15ã€œ1:17ã€æµ…ç…ã‚Šã¯æ¹¯æ¸©93Â±2â„ƒã€æ·±ç…ã‚Šã¯90Â±2â„ƒã€‚\n"
        "- ã‚¨ã‚¹ãƒ—ãƒ¬ãƒƒã‚½: 1:2å‰å¾Œ, 25â€“32ç§’, ãƒ–ãƒ«ãƒ¼ãƒ ä¸è¦ã€‚\n"
        "- ã‚°ãƒ©ã‚¤ãƒ³ãƒ‰ï¼šæµ…ç…ã‚Šâ†’ç´°ã‹ã‚/æ·±ç…ã‚Šâ†’ã‚„ã‚„ç²—ã‚ãŒå‚¾å‘ã€‚\n"
        "\n"
        "ã€å‡ºåŠ›å½¢å¼ã€‘\n"
        "ãƒ»å‰æè¦ç´„: ...\n"
        "ãƒ»æ¨å¥¨ãƒ¬ã‚·ãƒ”: ...\n"
        "ãƒ»å¾®èª¿æ•´Tips: â€¦ï¼ˆç®‡æ¡æ›¸ãï¼‰\n"
        "ãƒ»æ³¨æ„ç‚¹: ...\n"
        "ãƒ»ä»£æ›¿æ¡ˆ: ...\n"
    ),
    "æŠ¹èŒ¶ã®å°‚é–€å®¶ï¼ˆBï¼‰": (
        "ã‚ãªãŸã¯æŠ¹èŒ¶ã«ç²¾é€šã—ãŸèŒ¶ã®å°‚é–€å®¶ã§ã™ã€‚å“ç¨®ï¼ˆã¦ã‚“èŒ¶ç”±æ¥ï¼‰ã€ç‚¹ã¦æ–¹ï¼ˆè–„èŒ¶/æ¿ƒèŒ¶ï¼‰ã€"
        "é“å…·ï¼ˆèŒ¶ç¢—/èŒ¶ç­…/èŒ¶æ“/ãµã‚‹ã„ï¼‰ã€æ°´è³ªï¼ˆç¡¬åº¦/æ¸©åº¦ï¼‰ã‚’è¸ã¾ãˆã€å®¶åº­ã§ã‚‚å†ç¾ã—ã‚„ã™ã„æ‰‹é †ã§æ—¥æœ¬èªå›ç­”ã—ã¦ãã ã•ã„ã€‚"
        "\n\n"
        "ã€å›ç­”ãƒãƒªã‚·ãƒ¼ã€‘\n"
        "1) ã¾ãšç”¨é€”ç¢ºèªï¼ˆè–„èŒ¶/æ¿ƒèŒ¶/ãƒ©ãƒ†/è“å­ãƒšã‚¢ãƒªãƒ³ã‚°ï¼‰ã¨æŠ¹èŒ¶ã®ç­‰ç´šãƒ»é‡ãƒ»å™¨ã®æœ‰ç„¡ã‚’1è¡Œè¦ç´„ã€‚\n"
        "2) æ¨™æº–ãƒ¬ã‚·ãƒ”ï¼ˆæŠ¹èŒ¶g, æ¹¯ml, æ¹¯æ¸©â„ƒ, ãµã‚‹ã„æœ‰ç„¡, èŒ¶ç­…ã®å‹•ã‹ã—æ–¹, æ™‚é–“ï¼‰ã‚’æ•°å€¤ã§æç¤ºã€‚\n"
        "3) å‘³ã®èª¿æ•´ï¼ˆæ¸‹ã¿/æ—¨å‘³/é¦™ã‚Š/æ³¡ç«‹ã¡ï¼‰ã®ã‚³ãƒ„ã‚’ç®‡æ¡æ›¸ãã§3å€‹ä»¥å†…ã€‚\n"
        "4) æ°´ã¨ä¿å­˜ã®è¦ç‚¹ï¼ˆè»Ÿæ°´æ¨å¥¨, æ¹¯å†·ã¾ã—, ä½æ¸©å¤šæ¹¿å›é¿, è„±é…¸ç´ /é®å…‰å®¹å™¨ï¼‰ã‚’1è¡Œã§æ³¨æ„å–šèµ·ã€‚\n"
        "5) ã‚¢ãƒ¬ãƒ³ã‚¸ææ¡ˆï¼ˆãƒ©ãƒ†/ã‚½ãƒ¼ãƒ€/å’Œè“å­ãƒšã‚¢ï¼‰ã‚’1ã¤ã€‚\n"
        "\n"
        "ã€æ•°å€¤ã‚¬ã‚¤ãƒ‰ï¼ˆç›®å®‰ï¼‰ã€‘\n"
        "- è–„èŒ¶: æŠ¹èŒ¶2g / æ¹¯60â€“70ml / 80Â±2â„ƒã€èŒ¶ç­…ã¯ã€Må­—ã®ç´ æ—©ã„å‰å¾Œã€10â€“15ç§’ã€‚\n"
        "- æ¿ƒèŒ¶: æŠ¹èŒ¶3â€“4g / æ¹¯30â€“40ml / 75Â±2â„ƒã€ç·´ã‚Šã‚’æ„è­˜ã—æ³¡ã¯æ§ãˆã‚ã€‚\n"
        "- ãƒ©ãƒ†: æŠ¹èŒ¶2g / æ¹¯20mlã§æº¶ãã€ãƒŸãƒ«ã‚¯130â€“150mlï¼ˆ60â€“65â„ƒï¼‰ã€‚\n"
        "\n"
        "ã€å‡ºåŠ›å½¢å¼ã€‘\n"
        "ãƒ»å‰æè¦ç´„: ...\n"
        "ãƒ»æ¨™æº–ãƒ¬ã‚·ãƒ”: ...\n"
        "ãƒ»èª¿æ•´ã®ã‚³ãƒ„: â€¦ï¼ˆç®‡æ¡æ›¸ãï¼‰\n"
        "ãƒ»æ°´/ä¿å­˜ã®æ³¨æ„: ...\n"
        "ãƒ»ã‚¢ãƒ¬ãƒ³ã‚¸: ...\n"
    ),
}


# ======================
# LLMå‘¼ã³å‡ºã—é–¢æ•°ï¼ˆè¦ä»¶ï¼‰
# å…¥åŠ›: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ(str), é¸æŠãƒ­ãƒ¼ãƒ«(str)
# å‡ºåŠ›: å›ç­”ãƒ†ã‚­ã‚¹ãƒˆ(str), ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡(dict) ã‚‚ãŠã¾ã‘ã§è¿”ã™
# ======================
def get_llm_response(user_text: str, role_key: str) -> tuple[str, dict]:
    """
    æŒ‡å®šã®å°‚é–€å®¶ãƒ­ãƒ¼ãƒ«ã§ã€å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã«å›ç­”ã™ã‚‹ã€‚

    Parameters
    ----------
    user_text : str
        ç”»é¢ã®å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ ã‹ã‚‰ã®ãƒ†ã‚­ã‚¹ãƒˆ
    role_key : str
        ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã®é¸æŠè‚¢ã‚­ãƒ¼ï¼ˆROLE_SYSTEM_PROMPTSã®ã‚­ãƒ¼ï¼‰

    Returns
    -------
    tuple[str, dict]
        (å›ç­”ãƒ†ã‚­ã‚¹ãƒˆ, token_usageè¾æ›¸) ä¾‹: {"prompt_tokens": 123, "completion_tokens": 45, "total_tokens": 168}
    """
    if not os.environ.get("OPENAI_API_KEY"):
        raise RuntimeError("ç’°å¢ƒå¤‰æ•° OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

    system_prompt = ROLE_SYSTEM_PROMPTS.get(role_key, "You are a helpful assistant. Use Japanese.")

    llm = ChatOpenAI(model=MODEL_NAME, temperature=TEMPERATURE, api_key=os.getenv("OPENAI_API_KEY"))

    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_text),
    ]
    result = llm(messages)  # AIMessage ãŒè¿”ã‚‹
    content = result.content

    # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ï¼ˆã‚ã‚‹å ´åˆã®ã¿ï¼‰
    usage = {}
    try:
        meta = result.response_metadata or {}
        usage = (meta.get("token_usage") or {})  # {"prompt_tokens":..., "completion_tokens":..., "total_tokens":...}
    except Exception:
        pass

    return content, usage


# ======================
# Streamlit UI
# ======================
st.set_page_config(page_title="Expert Q&A (Coffee / Matcha)", page_icon="â˜•ï¸", layout="centered")

st.title("â˜•ï¸ğŸµ Expert Q&A (Coffee / Matcha) â€“ LangChain Lesson8ã‚¹ã‚¿ã‚¤ãƒ«")
st.caption(
    "æœ¬ã‚¢ãƒ—ãƒªã¯ã€å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’ LangChainï¼ˆChatOpenAI + System/HumanMessageï¼‰ã§ LLM ã«æ¸¡ã—ã€"
    "é¸æŠã—ãŸã€å°‚é–€å®¶ãƒ­ãƒ¼ãƒ«ã€ã¨ã—ã¦å›ç­”ã‚’è¿”ã—ã¾ã™ã€‚"
)

with st.expander("ã‚¢ãƒ—ãƒªã®æ¦‚è¦ã¨æ“ä½œæ–¹æ³•", expanded=True):
    st.markdown(
        """
**ä½¿ã„æ–¹**
1. ä¸‹ã®ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã§ã€Œå°‚é–€å®¶ãƒ­ãƒ¼ãƒ«ã€ã‚’é¸ã³ã¾ã™ï¼ˆA=ã‚³ãƒ¼ãƒ’ãƒ¼ã€B=æŠ¹èŒ¶ã€‚ä»–ã«ç´…èŒ¶ãƒ»ã‚«ãƒ•ã‚§çµŒå–¶ã‚‚ç”¨æ„ï¼‰ã€‚
2. ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢ã«è³ªå•ã‚„ç›¸è«‡ã‚’æ›¸ãã¾ã™ã€‚
3. **é€ä¿¡** ã‚’æŠ¼ã™ã¨ã€é¸æŠãƒ­ãƒ¼ãƒ«ã«å¿œã˜ãŸå£èª¿ãƒ»è¦³ç‚¹ã§å›ç­”ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚

**å†…éƒ¨æ§‹æˆ**
- `ChatOpenAI` ã¨ `SystemMessage` / `HumanMessage` ã‚’ç”¨ã„ã€ã‚·ãƒ³ãƒ—ãƒ«ãª **Model I/O** æ§‹æˆã€‚
- ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯ãƒ©ã‚¸ã‚ªã®é¸æŠã«å¿œã˜ã¦åˆ‡ã‚Šæ›¿ãˆã¦ã„ã¾ã™ã€‚
- æˆ»ã‚Šå€¤ã«å›ç­”ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ï¼ˆå¯èƒ½ãªå ´åˆï¼‰ã‚’å«ã‚ã¦ã„ã¾ã™ã€‚
        """
    )

with st.form("q_form", clear_on_submit=False):
    role = st.radio(
        "å°‚é–€å®¶ãƒ­ãƒ¼ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ï¼ˆA/Bã¯è¦ä»¶å›ºå®šãƒ»è¿½åŠ å…¥ã‚Šï¼‰",
        list(ROLE_SYSTEM_PROMPTS.keys()),
        index=0,
        horizontal=False,
    )
    user_text = st.text_area(
        "è³ªå•ãƒ»ç›¸è«‡å†…å®¹ï¼ˆä¾‹ï¼šæµ…ç…ã‚Šã‚¨ãƒã‚ªãƒ”ã‚¢ã®è¯ã‚„ã‹ãªé¦™ã‚Šã‚’æ´»ã‹ã™æŠ½å‡ºãƒ¬ã‚·ãƒ”ã‚’æ•™ãˆã¦ï¼‰",
        height=140,
        placeholder="ã“ã“ã«å…¥åŠ›ã—ã¦ãã ã•ã„â€¦",
    )
    submitted = st.form_submit_button("é€ä¿¡")

if submitted:
    if not user_text.strip():
        st.warning("å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ã™ã€‚å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    else:
        with st.spinner("LLMã«å•ã„åˆã‚ã›ä¸­â€¦"):
            try:
                answer, usage = get_llm_response(user_text.strip(), role)
                st.success("å›ç­”")
                st.write(answer)

                if usage:
                    c1, c2, c3 = st.columns(3)
                    c1.metric("prompt_tokens", usage.get("prompt_tokens", "â€”"))
                    c2.metric("completion_tokens", usage.get("completion_tokens", "â€”"))
                    c3.metric("total_tokens", usage.get("total_tokens", "â€”"))
            except Exception as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼: {e}\n\nOPENAI_API_KEY ã®è¨­å®šã‚„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯çŠ¶æ³ã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
